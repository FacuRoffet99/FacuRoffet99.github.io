Title: Losses
Date: 2025-12-22 12:40
Category: Deep Learning
Lang: es
Slug: losses
Author: Facundo Roffet

<!-- Hide default title -->
<style> h1.entry-title, h1.post-title, h1.title, h1:first-of-type {display: none;} </style>
<!-- Add custom title -->
<h2 style="text-align: center; font-size: 3em; color: rgba(12, 205, 76, 0.927);">Losses</h2>

<!---------------------------------------------------------------------------->

> Este post presenta una taxonomÃ­a estructurada de loss functions (funciones de coste) utilizadas en Deep Learning, organizÃ¡ndolas por tipo de tarea y mecanismo objetivo. La lista no es exhaustiva, ya que se centra en las losses mÃ¡s ampliamente adoptadas en la investigaciÃ³n moderna de computer vision y machine learning. Se omiten variantes de nicho o altamente especializadas, asÃ­ como losses especÃ­ficas para tareas sequence-to-sequence.

> La secciÃ³n sobre tareas generativas sirve como una visiÃ³n general amplia de tÃ©rminos en lugar de una lista granular de funciones independientes. Por el contrario, la secciÃ³n discriminativa proporciona un desglose mÃ¡s detallado de formulaciones especÃ­ficas.

> El objetivo de esta taxonomÃ­a es proporcionar una referencia intuitiva pero matemÃ¡ticamente rigurosa para seleccionar una loss adecuada en funciÃ³n de los requisitos geomÃ©tricos y probabilÃ­sticos de un problema especÃ­fico. La categorizaciÃ³n y las definiciones presentadas aquÃ­ se derivan principalmente de [Li et al. (2025)](https://doi.org/10.3390/math13152417).

<!---------------------------------------------------------------------------->

# this will be skipped

# Tareas discriminativas
Optimizan para $P(Y|X)$. Estas losses se centran en definir fronteras de decisiÃ³n o ajustar funciones que mapean inputs directamente a targets.

## D1. Losses de regresiÃ³n (continuas)
Los modelos de regresiÃ³n apuntan a predecir una variable dependiente continua $y$ basada en variables independientes $x$. Las losses en esta categorÃ­a son funciones de los residuos: la diferencia entre el valor observado $y$ y el valor predicho $\hat{y} = f(x)$.

### D1a. Basadas en magnitud (punto a punto)
Estas losses miden el error punto a punto entre la predicciÃ³n y el ground truth. GuÃ­an a los modelos para aproximar el valor objetivo minimizando la magnitud de estos errores.

**MAE (Mean Absolute Error)**  
$$ L_{MAE} = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i| $$
ğŸ—’ï¸ Calcula el promedio de las diferencias absolutas entre los valores predichos y los reales.  
ğŸ’¡ "No me importa la direcciÃ³n del error, solo decime en promedio por cuÃ¡ntas unidades le estoy errando. AdemÃ¡s, no me voy a volver loco por outliers masivos."   
âœ… Robusta a outliers (penalizaciÃ³n lineal).  
âœ… Proporciona una unidad fÃ­sica de error que es interpretable.  
âŒ Los gradientes no son diferenciables en 0, lo que puede complicar la convergencia cerca del Ã³ptimo.  

**MSE (Mean Squared Error)**  
$$ L_{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 $$
ğŸ—’ï¸ Calcula el promedio de las diferencias al cuadrado. Elevar el error al cuadrado asegura positividad y penaliza los errores grandes desproporcionadamente mÃ¡s que los pequeÃ±os.  
ğŸ’¡ "Los errores chicos estÃ¡n bien, pero si le errÃ¡s por mucho te voy a castigar severamente para asegurar que no lo vuelvas a hacer."   
âœ… Diferenciable en todas partes (descenso de gradiente suave).  
âœ… Converge mÃ¡s rÃ¡pido que MAE cuando estÃ¡ cerca del mÃ­nimo.  
âŒ Altamente sensible a outliers, un solo punto de datos malo puede sesgar todo el modelo.  
â†”ï¸ Variante RMSE: Convierte el error nuevamente a las unidades originales de la variable objetivo tomando la raÃ­z cuadrada, haciÃ©ndolo mÃ¡s fÃ¡cil de interpretar.  
â†”ï¸ Variante RMSLE: Hace que la loss sea sensible a errores relativos en lugar de absolutos y penaliza la subestimaciÃ³n mÃ¡s que la sobreestimaciÃ³n.  

**Log-Cosh**  
$$ L_{LogCosh} = \frac{1}{N} \sum_{i=1}^{N} \log(\cosh(\hat{y}_i - y_i)) $$
ğŸ—’ï¸ Calcula el logaritmo del coseno hiperbÃ³lico del error de predicciÃ³n. Se aproxima a $\frac{x^2}{2}$ para $x$ pequeÃ±os y a $|x| - \log(2)$ para $x$ grandes.  
ğŸ’¡ "ActuÃ¡ como MSE cuando el error sea chico para hacer un fine-tuning suave, pero pasate al comportamiento MAE cuando el error sea enorme asÃ­ los outliers no te distraen."   
âœ… Combina lo mejor de ambos mundos: robusta a outliers (como MAE) y diferenciable en todas partes (como MSE).  
âŒ Computacionalmente mÃ¡s costosa que las losses polinÃ³micas simples.  

**Huber**  
$$
L_{Huber} = 
\begin{cases}
  \frac{1}{2}(y - \hat{y})^2 & \text{si } |y - \hat{y}| \le \delta \\\\
  \delta \cdot (|y - \hat{y}| - \frac{1}{2}\delta) & \text{en otro caso}
\end{cases}
$$
ğŸ—’ï¸ Una funciÃ³n a trozos que es cuadrÃ¡tica para errores pequeÃ±os (por debajo de un umbral $\delta$) y lineal para errores grandes. Requiere un hiperparÃ¡metro $\delta$ para definir el punto de transiciÃ³n.  
ğŸ’¡ "No entres en pÃ¡nico si un punto de datos estÃ¡ muy lejos, simplemente traelo linealmente. Pero una vez que te acerques, curvÃ¡ la loss para aterrizar el aviÃ³n suavemente."   
âœ… Robusta a outliers manteniendo la diferenciabilidad en 0.  
âŒ Introduce un hiperparÃ¡metro ($\delta$) que debe ser ajustado.  

**Quantile**
$$
L_{Quantile} = 
\begin{cases}
  \tau |\hat{y}_i - y_i| & \text{si } |y - \hat{y}| \le \delta \\\\
  (1-\tau)|\hat{y}_i - y_i| & \text{en otro caso}
\end{cases}
$$ 
ğŸ—’ï¸ Una extensiÃ³n de MAE que aplica diferentes penalizaciones a la sobreestimaciÃ³n y subestimaciÃ³n basada en un cuantil elegido $\tau$ (entre 0 y 1). Se usa para predecir intervalos de predicciÃ³n en lugar de una media Ãºnica.  
ğŸ’¡ "No quiero solo el resultado promedio, quiero estar 90% seguro de que el valor real estÃ¡ por debajo de mi lÃ­nea de predicciÃ³n."   
âœ… Permite la estimaciÃ³n de incertidumbre y la construcciÃ³n de intervalos de confianza.  
âŒ MÃ¡s difÃ­cil de entrenar, la convergencia puede ser mÃ¡s lenta que con MSE/MAE estÃ¡ndar.  

### D1b. Geometry-aware (bounding boxes)
En tareas de detecciÃ³n de objetos, el objetivo de la regresiÃ³n de bounding boxes es lograr una alineaciÃ³n geomÃ©trica entre la caja predicha y el ground truth. A diferencia de las losses basadas en magnitud, las losses geomÃ©tricas no tratan las coordenadas de forma aislada. En su lugar ven la caja como una entidad geomÃ©trica unificada, optimizando la relaciÃ³n espacial (superposiciÃ³n, distancia y forma) entre la predicciÃ³n y el ground truth.

**IoU (Intersection over Union)**  
$$ L_{IoU} = 1 - \frac{|B \cap B^{gt}|}{|B \cup B^{gt}|} $$
ğŸ—’ï¸ Mide el Ã¡rea de superposiciÃ³n entre la caja predicha $B$ y la caja de ground truth $B^{gt}$ dividida por su Ã¡rea de uniÃ³n.  
ğŸ’¡ "No me importa dÃ³nde estÃ¡n los pÃ­xeles exactamente, solo asegurate de que los dos cuadrados se superpongan lo mÃ¡ximo posible."   
âœ… Invariante a la escala del problema (una caja chica y una grande con el mismo % de superposiciÃ³n tienen la misma loss).  
âŒ Si las cajas no se superponen: IoU es 0, el gradiente es 0, y el modelo deja de aprender completamente.  
âŒ Si las cajas se superponen completamente: IoU es 1, y el gradiente se vuelve 0 nuevamente.  

**GIoU (Generalized IoU)**  
$$ L_{GIoU} = 1 - IoU + \frac{|C \setminus (B \cup B^{gt})|}{|C|} $$
Donde $C$ es la caja convexa mÃ¡s pequeÃ±a que cubre tanto a $B$ como a $B^{gt}$.  
ğŸ—’ï¸ Agrega un tÃ©rmino de penalizaciÃ³n basado en el espacio vacÃ­o dentro de la caja envolvente mÃ¡s pequeÃ±a $C$. Esto asegura que existan gradientes incluso cuando las cajas no se superponen.  
ğŸ’¡ "Si las cajas no se tocan, movÃ© la predicciÃ³n hacia el target para minimizar el espacio vacÃ­o entre ellas."   
âœ… Resuelve el problema de desvanecimiento de gradiente para cajas que no se superponen.  
âŒ No resuelve el problema de desvanecimiento de gradiente para cajas completamente superpuestas.  
âŒ La convergencia es lenta, tiende a expandir la caja predicha para cubrir el target primero antes de encogerse para encajar.  

**DIoU (Distance IoU)**  
$$ L_{DIoU} = 1 - IoU + \frac{\rho^2(b, b^{gt})}{c^2} $$
Donde $\rho$ es la distancia Euclidiana, $b$ y $b^{gt}$ son los puntos centrales, y $c$ es la longitud diagonal de la caja envolvente.   
ğŸ—’ï¸ Agrega una penalizaciÃ³n que minimiza la distancia normalizada entre los puntos centrales de las dos cajas.  
ğŸ’¡ "No solo superpongas, apuntale al medio. AlineÃ¡ los centros de las cajas directamente."   
âœ… Converge mucho mÃ¡s rÃ¡pido que GIoU porque minimiza la distancia directamente en lugar del Ã¡rea.  
âœ… Resuelve completamente el problema de desvanecimiento de gradiente.  
âŒ No considera la relaciÃ³n de aspecto de las cajas.  

**CIoU (Complete IoU)**  
$$L_{CIoU} = 1 - IoU + \frac{\rho^2(b, b^{gt})}{c^2} + \alpha v$$
Donde $v$ mide la consistencia de la relaciÃ³n de aspecto y $\alpha$ es un parÃ¡metro de ponderaciÃ³n.  
ğŸ—’ï¸ Extiende DIoU agregando un tÃ©rmino para asegurar que la relaciÃ³n de aspecto de la predicciÃ³n coincida con el target.  
ğŸ’¡ "SuperponÃ©, pegale al centro y asegurate de no estar dibujando un rectÃ¡ngulo alto cuando deberÃ­a ser uno ancho."   
âœ… Considera todos los factores geomÃ©tricos: Ã¡rea de superposiciÃ³n, distancia del punto central y relaciÃ³n de aspecto.  
âŒ El tÃ©rmino de relaciÃ³n de aspecto $v$ es complejo y los gradientes a veces pueden ser inestables dependiendo de la implementaciÃ³n.  

**EIoU (Efficient IoU)**  
$$L_{EIoU} = 1 - IoU + \frac{\rho^2(b, b^{gt})}{c^2} + \frac{\rho^2(w, w^{gt})}{C_w^2} + \frac{\rho^2(h, h^{gt})}{C_h^2}$$
Donde $w,h$ son ancho/alto y $C_w, C_h$ son el ancho/alto de la caja envolvente.  
ğŸ—’ï¸ Mejora CIoU dividiendo el tÃ©rmino de relaciÃ³n de aspecto en penalizaciones separadas para las diferencias de ancho y alto.  
ğŸ’¡ "La matemÃ¡tica de CIoU es complicada, mejor vamos a medir el error de ancho y el error de alto por separado."   
âœ… Convergencia mÃ¡s rÃ¡pida y mejor precisiÃ³n de localizaciÃ³n que CIoU.  
âœ… Resuelve la ambigÃ¼edad en CIoU donde diferentes pares $w/h$ podÃ­an producir la misma penalizaciÃ³n de relaciÃ³n de aspecto.  

**SIoU (Scylla-IoU)**  
$$L_{SIoU} = 1 - IoU + \frac{\Delta + \Omega}{2}$$
Donde $\Delta$ es el costo de distancia y $\Omega$ es el costo de forma.  
ğŸ—’ï¸ Introduce un costo angular a la regresiÃ³n. Considera el Ã¡ngulo vectorial entre el centro de la caja predicha y el ground truth. Prioriza alinear la caja al eje mÃ¡s cercano (X o Y) para minimizar la libertad de movimiento.  
ğŸ’¡ "DejÃ¡ de dar vueltas en diagonal. Movete estrictamente en horizontal o vertical para alinearte con el target primero, despuÃ©s ajustÃ¡ el tamaÃ±o."   
âœ… Converge mÃ¡s rÃ¡pido que CIoU y EIoU reduciendo la oscilaciÃ³n de la caja durante el entrenamiento.  
âŒ Computacionalmente un poco mÃ¡s pesada debido al cÃ¡lculo de componentes trigonomÃ©tricos (seno inverso).  

## D2. Losses de clasificaciÃ³n (discretas)
La clasificaciÃ³n es un subconjunto de tareas de aprendizaje supervisado donde el objetivo es asignar un input $x$ a una de $K$ clases discretas.

### D2a. Basadas en margen (fronteras de decisiÃ³n)
Las losses de margen introducen un parÃ¡metro de umbral para imponer una separaciÃ³n mÃ­nima entre el puntaje predicho y la clase correcta. Obligan al modelo no solo a clasificar correctamente, sino a hacerlo con alta confianza manteniendo una 'distancia segura' de la frontera de decisiÃ³n.

**Hinge**  
$$L_{Hinge} = \max(0, 1 - y_i \hat{y}_i)$$
ğŸ—’ï¸ La loss estÃ¡ndar para Support Vector Machines (SVMs). Solo penaliza al modelo si el puntaje de la clase correcta no es suficientemente mÃ¡s alto que el margen. Si la predicciÃ³n es correcta y segura ($y \hat{y} \ge 1$), la loss es cero.  
ğŸ’¡ "No quiero solo que tengas razÃ³n, quiero que tengas razÃ³n por un margen amplio. Si apenas pasÃ¡s la lÃ­nea de meta, igual te voy a penalizar."   
âœ… Los puntos que se clasifican correctamente con alta confianza tienen gradientes 0 y no afectan la actualizaciÃ³n del modelo, ahorrando recursos computacionales.  
âŒ La funciÃ³n no es diferenciable en $y\hat{y}=1$, requiriendo mÃ©todos de optimizaciÃ³n de sub-gradiente.  
â†”ï¸ Variante Squared Hinge: Diferenciable pero sensible a outliers.  
â†”ï¸ Variante Quadratic Smoothed Hinge: Lineal para errores grandes para mantener robustez, y cuadrÃ¡tica cerca de la frontera del margen para asegurar diferenciabilidad.  
â†”ï¸ Variante Ramp: Limita la loss para ignorar outliers extremos.  

**Exponential**  
$$L_{Exp} = e^{-y_i \hat{y}_i}$$
ğŸ—’ï¸ Utilizada principalmente en algoritmos de boosting como AdaBoost. Aplica una penalizaciÃ³n exponencial a mÃ¡rgenes negativos (clasificaciones incorrectas).  
ğŸ’¡ "Si te equivocÃ¡s en un ejemplo difÃ­cil, la penalizaciÃ³n va a ser masiva. Te tenÃ©s que obsesionar con los puntos de datos mÃ¡s difÃ­ciles."   
âœ… Fuerza al modelo a enfocarse intensamente en los ejemplos en los que se estÃ¡ equivocando actualmente.  
âœ… Diferenciable y convexa.  
âŒ Debido a que la penalizaciÃ³n crece exponencialmente, un solo outlier mal etiquetado puede dominar el gradiente y arruinar el proceso de entrenamiento.  

### D2b. ProbabilÃ­sticas (divergencia de distribuciÃ³n)
Sea $q$ la distribuciÃ³n de probabilidad verdadera del dataset y $p_{\theta}$ la distribuciÃ³n predicha generada por el modelo. Las losses probabilÃ­sticas miden la divergencia (distancia) entre $q$ y $p_{\theta}$. Al minimizar esta divergencia, la distribuciÃ³n de salida del modelo converge hacia el ground truth.

**CE (Cross-Entropy)**  
$$L_{CE} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{k=1}^{K} y_{i,k} \log(\hat{y}_{i,k})$$
ğŸ—’ï¸ Mide la diferencia de informaciÃ³n entre la distribuciÃ³n predicha y la distribuciÃ³n verdadera. Cuando los targets estÃ¡n codificados en one-hot, minimizar CE es matemÃ¡ticamente equivalente a maximizar la verosimilitud (likelihood) de la clase correcta.  
ğŸ’¡ "Si la imagen es un gato, quiero que la probabilidad de 'gato' sea 1.0. Cada pedacito de masa de probabilidad asignada a 'perro' o 'pÃ¡jaro' aumenta la penalizaciÃ³n."   
âœ… La loss por defecto para clasificaciÃ³n, diferenciable y rigurosamente basada en TeorÃ­a de la InformaciÃ³n.  
âŒ Dominada por clases mayoritarias si los datos estÃ¡n desbalanceados.  
âŒ Dominada por ejemplos fÃ¡ciles (fondo) en tareas de detecciÃ³n densa.  
â†”ï¸ Variante Weighted CE: Multiplica la loss de la clase $k$ por un peso $\alpha_k$ (usualmente inverso a la frecuencia de clase o al nÃºmero efectivo de muestras).  
â†”ï¸ Variante Label Smoothing: Cambia el target $y=1$ a $y=1-\epsilon$ y $y=0$ a $y=\frac{\epsilon}{K-1}$ para prevenir el overfitting.  

**Focal**  
$$L_{Focal} = -\frac{1}{N} \sum_{i=1}^{N} \alpha (1 - \hat{p}_i)^\gamma \log(\hat{p}_i)$$
ğŸ—’ï¸ Agrega un factor modulador $(1 - \hat{p}_i)^\gamma$ a la Cross-Entropy estÃ¡ndar. Si una muestra ya estÃ¡ bien clasificada (ej., $\hat{p}_i = 0.9$), el factor se acerca a 0, silenciando efectivamente la loss para ese ejemplo.  
ğŸ’¡ "No me importa el cielo de fondo que ya identificaste correctamente 1.000 veces. Concentrate en ese Ãºnico pÃ­xel difÃ­cil que parece un peatÃ³n."   
âœ… Resuelve el problema de desbalance de clases sin sobremuestreo manual.  
âœ… El estÃ¡ndar para detecciÃ³n de objetos densa.  
âŒ Requiere ajustar dos hiperparÃ¡metros ($\alpha$ y $\gamma$) que pueden ser sensibles al dataset.  

**GHM (Gradient Harmonized Mechanism)**  
$$L_{GHM} = \sum_{i=1}^{N} \frac{L_{CE}}{GD(g_i)}$$
Donde $g_i$ es la norma del gradiente y $GD$ es la densidad de gradiente (una medida de cuÃ¡ntas muestras tienen ese mismo gradiente).  
ğŸ—’ï¸ Una alternativa avanzada a Focal loss. Observa que los ejemplos fÃ¡ciles y los outliers tienen comportamientos de gradiente distintos. Armoniza el entrenamiento normalizando la loss basada en la densidad de gradientes. Si un millÃ³n de ejemplos producen el mismo gradiente pequeÃ±o (fondo fÃ¡cil), su contribuciÃ³n se divide por un factor de densidad grande.  
ğŸ’¡ "Si todos los demÃ¡s estÃ¡n gritando lo mismo, voy a bajarle el volumen a ese grupo. Solo quiero escuchar los errores Ãºnicos/raros."   
âœ… No requiere ajuste manual de $\alpha$ o $\gamma$ como Focal loss, se adapta a la dinÃ¡mica de los datos de entrenamiento.  
âŒ El costo computacional aumenta: requiere calcular un histograma de gradientes a travÃ©s del batch/dataset durante el entrenamiento.  

**Poly**  

$$
L_{Poly} = -\sum_{j=1}^{\infty} \alpha_j (1 - \hat{p}_t)^j 
$$

ğŸ—’ï¸ Trata a la Cross-Entropy como una expansiÃ³n en serie de Taylor y la generaliza, permitiendo el ajuste de los coeficientes polinÃ³micos principales $\alpha_j$ para cambiar estructuralmente cÃ³mo se comporta la loss (en lugar de tenerlos fijos en $1/j$). En la prÃ¡ctica, usualmente solo se modifica el coeficiente principal ($\epsilon_1$): $L_{Poly-1} = L_{CE} + \epsilon_1 (1 - \hat{p}_t)$.  
ğŸ’¡ "Cross-Entropy es una curva fija, pero podemos cambiarle la forma si lo necesitamos."   
âœ… FunciÃ³n generalizada que abarca Cross-Entropy y Focal Loss como casos especiales.  
âœ… Poly-1 suele superar a Focal/CE ajustando un solo parÃ¡metro, con un costo computacional adicional despreciable.  
âŒ Introduce un hiperparÃ¡metro no estÃ¡ndar que debe encontrarse mediante grid search, ya que no hay un valor por defecto universal.  

### D2c. Overlap-based (segmentaciÃ³n)
En segmentaciÃ³n semÃ¡ntica, la clasificaciÃ³n ocurre a nivel de pÃ­xel. Sin embargo, las losses pixel-wise a menudo tienen problemas cuando el objeto objetivo ocupa solo una pequeÃ±a fracciÃ³n de la imagen. Las losses basadas en superposiciÃ³n abordan esto optimizando directamente la intersecciÃ³n entre el mapa de segmentaciÃ³n predicho y el ground truth, priorizando la alineaciÃ³n global de la forma sobre la precisiÃ³n individual de los pÃ­xeles.

**Tversky**  
$$L_{Tversky} = 1 - \frac{\sum \hat{y} y}{\sum \hat{y} y + \alpha \sum (1-y)\hat{y} + \beta \sum y(1-\hat{y})}$$
Donde $y$ es el target, $\hat{y}$ es la predicciÃ³n, $\alpha$ controla la penalizaciÃ³n para falsos positivos, y $\beta$ controla la penalizaciÃ³n para falsos negativos.  
ğŸ—’ï¸ Una generalizaciÃ³n del coeficiente Dice (cuando $\alpha = \beta = 0.5$). Permite cambiar el balance entre precisiÃ³n (evitar falsos positivos) y recall (evitar falsos negativos).  
ğŸ’¡ "Si encontrar el tumor es crÃ­tico y no podemos permitirnos perderlo, ponÃ© $\beta$ mÃ¡s alto que $\alpha$ para penalizar a los pÃ­xeles perdidos mÃ¡s que los que sobran."   
âœ… Mucho mejor que Cross-Entropy para manejar desbalance en objetos pequeÃ±os.  
âœ… Los parÃ¡metros $\alpha$ y $\beta$ dan flexibilidad para ajustar el trade-off en base a las necesidades clÃ­nicas o de negocio.  
âŒ Puede ser inestable durante las etapas tempranas del entrenamiento comparada con CE pixel-wise.  
â†”ï¸ Variante Focal Tversky: Aplica el mecanismo de Focal loss al Ã­ndice Tversky.  

**Sensitivity-Specificity**  
$$L_{SS} = w \cdot \frac{\sum (y-\hat{y})^2 y}{\sum y} + (1-w) \cdot \frac{\sum (y-\hat{y})^2 (1-y)}{\sum (1-y)}$$
ğŸ—’ï¸ Optimiza explÃ­citamente la suma ponderada de los errores cuadrÃ¡ticos para la clase positiva (sensibilidad) y la clase negativa (especificidad). Asegura que el modelo no logre alta precisiÃ³n simplemente ignorando el fondo o el primer plano.  
ğŸ’¡ "Necesito que seas bueno encontrando el objeto, pero igual de bueno NO encontrando el objeto donde no existe. BalanceÃ¡ tu entusiasmo."   
âœ… Aborda tanto la sobre-segmentaciÃ³n (incluir demasiado fondo) como la sub-segmentaciÃ³n (perder partes del objeto).  
âœ… Apropiada para contextos mÃ©dicos donde la especificidad es tan vital como la sensibilidad.  
âŒ Altamente sensible al parÃ¡metro de peso $w$. Si se configura incorrectamente, el modelo puede colapsar prediciendo solo el fondo o solo el primer plano.  

## D3. Losses mÃ©tricas (espacio de embeddings)
El objetivo del aprendizaje mÃ©trico es aprender las distancias relativas entre inputs en lugar de predecir una etiqueta o valor especÃ­fico. Las losses mÃ©tricas operan sobre pares (o tripletes) de instancias de datos, extrayendo un embedding para cada una. Una mÃ©trica de distancia mide la similitud entre estas representaciones. El modelo se entrena para minimizar la distancia entre representaciones de inputs similares y maximizar la distancia entre los disÃ­miles, estructurando el espacio de embeddings de manera significativa.

### D3a. Distancia EuclÃ­dea
Estas losses usan directamente la distancia geomÃ©trica en el espacio de embeddings como el objetivo de optimizaciÃ³n.

**Contrastive Loss**  
$$L_{Contrastive} = \frac{1}{2} \sum_{i=1}^{N} [Y_iD_i^2 + (1-Y_i) \max(0, m - D_i)^2]$$
Donde $D = ||f(x_1) - f(x_2)||_2$ es la distancia EuclÃ­dea entre el par de muestras, $Y=1$ implica misma clase, $Y=0$ implica clase diferente, y $m$ es el margen.  
ğŸ—’ï¸ Toma pares de muestras. Si pertenecen a la misma clase, minimiza su distancia. Si pertenecen a clases diferentes, las empuja hasta que estÃ©n al menos a un margen $m$ de distancia.  
ğŸ’¡ "Si son gemelos, abrÃ¡cense. Si son desconocidos, alÃ©jense hasta que tengan al menos 1 metro de espacio personal entre ustedes."   
âœ… Es el enfoque fundacional simple para el aprendizaje mÃ©trico.  
âŒ DifÃ­cil ajustar el margen. Si $m$ es muy chico, los clusters se superponen; si es muy grande, el entrenamiento se vuelve inestable.  

**Triplet**  
$$L_{Triplet} = \sum_{i=1}^{N} \max(0, D(a_i, p_i)^2 - D(a_i, n_i)^2 + m)$$
Donde $a$ es ancla, $p$ es positivo (misma clase), $n$ es negativo (clase diferente), y $m$ es margen.  
ğŸ—’ï¸ Toma tres muestras a la vez: un ancla, un positivo y un negativo. Asegura que el ancla estÃ© mÃ¡s cerca del positivo que del negativo por al menos un margen $m$.  
ğŸ’¡ "No me importa exactamente dÃ³nde estÃ¡ el ancla, siempre y cuando su amigo (positivo) estÃ© mÃ¡s cerca de ella que su enemigo (negativo)."   
âœ… MÃ¡s flexible que Contrastive loss porque relaja la restricciÃ³n sobre distancias absolutas, solo importa el ranking relativo.  
âŒ Requiere encontrar negativos que estÃ©n actualmente mÃ¡s cerca que los positivos. Si se eligen negativos al azar, la loss suele ser 0 y el modelo no aprende nada.  

**InfoNCE (Information Noise-Contrastive Estimation)**  
ğŸ—’ï¸ Trata la tarea como un problema de clasificaciÃ³n: "Entre este batch de $K$ negativos y 1 positivo, identificÃ¡ el positivo". Maximiza la informaciÃ³n mutua entre la query y la key positiva.  
ğŸ’¡ "AcÃ¡ tenÃ©s una foto de un perro y 1.000 fotos de otras cosas. Â¿PodÃ©s elegir el perro correcto de esta rueda de reconocimiento?"   
âœ… Aprende de un positivo y muchos negativos simultÃ¡neamente, proporcionando una seÃ±al de gradiente mucho mÃ¡s rica que Triplet.  
âœ… Es el backbone estÃ¡ndar para el aprendizaje de representaciÃ³n auto-supervisado moderno.  
âŒ A menudo requiere un batch size muy grande para tener suficientes negativos difÃ­ciles y funcionar eficazmente.  

### D3b. Margen angular
Las losses basadas en mÃ¡rgenes angulares o coseno no optimizan directamente la posiciÃ³n absoluta y la distancia en el espacio de features. En cambio, se centran en las fronteras angulares entre clases proyectando features en una hiperesfera y optimizando la similitud coseno entre vectores de features y centros de clases.

**A-Softmax (Angular Softmax / SphereFace)**  
$$L_{Sphere} = -\log \frac{e^{||x_i|| \psi(\theta_{y_i})}}{e^{||x_i|| \psi(\theta_{y_i})} + \sum_{j \neq y_i} e^{||x_i|| \cos(\theta_j)}}$$
Donde $\psi(\theta)$ es una funciÃ³n monotÃ³nica que reemplaza $\cos(\theta)$ con $\cos(m\theta)$.  
ğŸ—’ï¸ La primera loss angular. Introduce un margen angular multiplicativo $m$, y fuerza al Ã¡ngulo de la clase correcta a ser $m$ veces mÃ¡s pequeÃ±o que el Ã¡ngulo de cualquier clase incorrecta.  
ğŸ’¡ "Si el Ã¡ngulo al centro de tu clase es 10 grados, voy a hacer de cuenta que en realidad es 40 grados ($m=4$). TenÃ©s que trabajar 4 veces mÃ¡s duro para demostrar que pertenecÃ©s ahÃ­."   
âœ… Pionera en el concepto de mÃ¡rgenes angulares, demostrando que las restricciones geomÃ©tricas en la hiperesfera mejoran significativamente la discriminaciÃ³n de features.  
âŒ La optimizaciÃ³n es difÃ­cil y requiere un annealing complejo del hiperparÃ¡metro $\lambda$ para converger.  

**AM-Softmax (Additive Margin Softmax / CosFace)**  
$$L_{Cos} = -\log (\frac{e^{s(\cos(\theta_{y_i}) - m)}}{e^{s(\cos(\theta_{y_i}) - m)} + \sum_{j \neq y_i} e^{s \cos(\theta_j)}})$$
Donde $s$ es un factor de escala y $m$ es un margen de coseno aditivo.  
ğŸ—’ï¸ Simplifica SphereFace moviendo el margen $m$ fuera de la funciÃ³n coseno. Resta un margen $m$ directamente del valor de similitud coseno.  
ğŸ’¡ "La Softmax estÃ¡ndar es muy permisiva. Le voy a restar 0.3 a tu puntaje de similitud. Efectivamente necesitÃ¡s un puntaje de 1.3 para obtener un 1.0 perfecto. Â¡Esforzate mÃ¡s!"   
âœ… Mucho mÃ¡s fÃ¡cil de implementar y entrenar que SphereFace.  
âœ… Es mÃ¡s interpretable ya que optimiza directamente el gap de similitud coseno.  

**Additive Angular Margin (ArcFace)**  
$$L_{Arc} = -\log (\frac{e^{s \cos(\theta_{y_i} + m)}}{e^{s \cos(\theta_{y_i} + m)} + \sum_{j \neq y_i} e^{s \cos(\theta_j)}})$$
Donde $m$ es un margen angular aditivo sumado dentro del coseno.  
ğŸ—’ï¸ Agrega el margen $m$ dentro del tÃ©rmino coseno, lo que corresponde a una penalizaciÃ³n de distancia geodÃ©sica directa en la hiperesfera.  
ğŸ’¡ "Imaginate que las clases son paÃ­ses en un globo terrÃ¡queo. ArcFace dibuja fronteras estrictas con una zona buffer entre cada paÃ­s directamente sobre la superficie de la esfera."   
âœ… El margen tiene una correspondencia constante con la longitud de arco en la hiperesfera.  
âœ… Es state-of-the-art para reconocimiento facial.  
âŒ Requiere un ajuste cuidadoso de la escala $s$ y el margen $m$ dependiendo del ruido del dataset.  

**Quality Adaptive Margin Softmax (AdaFace)**  
$$L_{Ada} = -\log (\frac{e^{s \cos(\theta_{y_i} + g_{angle})}}{e^{s \cos(\theta_{y_i} + g_{angle})} + \sum_{j \neq y_i} e^{s \cos(\theta_j)}})$$
Donde $g_{angle}$ es una funciÃ³n de margen que se adapta basada en la calidad de la imagen (norma del feature $||\hat{z}_i||$).  
ğŸ—’ï¸ Adapta el margen basado en la calidad de la imagen de entrada. Aplica un margen estricto a imÃ¡genes de alta calidad y un margen relajado a imÃ¡genes de baja calidad para evitar que el modelo haga overfitting al ruido.  
ğŸ’¡ "Si la foto es HD, espero perfecciÃ³n. Si la foto es un cuadro borroso de una cÃ¡mara de seguridad, voy a ser mÃ¡s suave con vos para que no te confundas tratando de aprender ruido."   
âœ… Estado del arte para reconocimiento facial no restringido (ej., vigilancia, baja resoluciÃ³n).  
âœ… Evita que el modelo se trabe tratando de optimizar muestras irreconociblemente.  
âŒ Introduce complejidad en la implementaciÃ³n y depende de la asunciÃ³n de que la norma del feature correlaciona con la calidad de la imagen (lo cual suele ser cierto, pero no siempre).  

<!---------------------------------------------------------------------------->

# Tareas generativas
Optimizan para $P(X)$ o $P(X,Y)$. Las losses para estas tareas se enfocan en aprender la distribuciÃ³n de datos subyacente para generar nuevas muestras o reconstruir inputs. Estas funciones objetivo suelen ser compuestas, mezclando mÃºltiples tÃ©rminos (reconstrucciÃ³n, coincidencia de distribuciÃ³n, calidad perceptual) para lograr resultados realistas.

## G1. TÃ©rminos de reconstrucciÃ³n (element-wise)
Estos tÃ©rminos aseguran fidelidad midiendo la diferencia directa entre el input original $x$ y el output reconstruido/generado $\hat{x}$.

**MSE (Mean Squared Error)**  
ğŸ—’ï¸ Es la misma funciÃ³n usada para tareas discriminativas. Para tareas generativas, esto actÃºa como el tÃ©rmino de "fidelidad".  
ğŸ’¡ "La imagen generada se tiene que ver exactamente como el input, pÃ­xel por pÃ­xel."   
âœ… Simple de implementar y garantiza teÃ³ricamente el PSNR (Peak Signal-to-Noise Ratio) mÃ¡s alto.  
âŒ En contextos generativos, MSE puro tiende a producir imÃ¡genes borrosas porque promedia los detalles de alta frecuencia.  

## G2. TÃ©rminos de coincidencia de distribuciÃ³n (divergencias)
Estos tÃ©rminos minimizan la discrepancia estadÃ­stica entre la distribuciÃ³n aprendida $P_g$ y la distribuciÃ³n de datos real $P_{data}$. Son centrales en GANs y Variational Autoencoders (VAEs).

**Minimax (GAN loss)**  
ğŸ—’ï¸ Un juego de suma cero entre dos redes: un generador ($G$) intenta engaÃ±ar al discriminador, y un discriminador ($D$) intenta distinguir real de falso.  
ğŸ’¡ "Generador: Apuesto a que te puedo engaÃ±ar. Discriminador: No, no podÃ©s, voy a detectar el falso."   
âœ… Produce detalles muy nÃ­tidos y realistas comparado con MSE.  
âŒ Muy difÃ­cil de entrenar.  

**Wasserstein Distance (WGAN loss)**  
ğŸ—’ï¸ Calcula el "trabajo" mÃ­nimo (masa Ã— distancia) requerido para transformar una distribuciÃ³n en otra. A diferencia de la loss GAN estÃ¡ndar, el discriminador (ahora llamado crÃ­tico) emite un puntaje crudo, no una probabilidad.  
ğŸ’¡ "En lugar de preguntar 'Â¿Verdadero o Falso?', mejor preguntÃ¡ 'Â¿QuÃ© tan real es esto?' para dejarle saber al generador exactamente cuÃ¡n lejos estÃ¡ del target, incluso si actualmente estÃ¡ fallando por completo."   
âœ… Proporciona gradientes significativos incluso cuando las distribuciones real y falsa no se superponen en absoluto, resolviendo el problema de desvanecimiento de gradiente de las GANs estÃ¡ndar.  
âœ… El valor de la loss correlaciona linealmente con la calidad visual de las imÃ¡genes generadas, lo cual no es cierto para la loss GAN estÃ¡ndar.  
âŒ Requiere imponer continuidad 1-Lipschitz (el gradiente no puede cambiar muy rÃ¡pido), lo cual es difÃ­cil de implementar.  

**KL (Kullback-Leibler Divergence)**  
$$ L_{KL} = \sum P(x) \log (\frac{P(x)}{Q(x)}) $$
ğŸ—’ï¸ Mide cuÃ¡nta informaciÃ³n se pierde cuando la distribuciÃ³n $Q$ se usa para aproximar $P$. En VAEs, fuerza al espacio latente aprendido a seguir una distribuciÃ³n Gaussiana estÃ¡ndar.  
ğŸ’¡ "MantenÃ© tu espacio latente organizado como una campana de Gauss estÃ¡ndar asÃ­ podemos muestrear de Ã©l fÃ¡cilmente despuÃ©s."   
âœ… Fuerza a las variables latentes aprendidas a seguir una distribuciÃ³n tratable (usualmente Gaussiana Unitaria), asegurando que el espacio latente sea suave y continuo.  
âŒ La restricciÃ³n Gaussiana estricta a menudo resulta en salidas sobre-regularizadas y borrosas.  

**Sinkhorn Divergence**  
$$L_{Sinkhorn} = \min_{\pi} \sum_{i,j} C_{i,j} \pi_{i,j} + \epsilon H(\pi)$$
Donde $C$ es la matriz de costo, $\pi$ es el plan de transporte, y $H$ es la entropÃ­a de regularizaciÃ³n.  
ğŸ—’ï¸ Agrega un tÃ©rmino de regularizaciÃ³n entrÃ³pica al problema de transporte Ã³ptimo. Esto permite que la distancia Wasserstein sea calculada mucho mÃ¡s rÃ¡pido usando el algoritmo Sinkhorn-Knopp.  
ğŸ’¡ "Calcular el plan perfecto de movimiento de tierra es difÃ­cil. Si permitimos un poco de aleatoriedad en a dÃ³nde va la tierra, podemos resolver la matemÃ¡tica 100 veces mÃ¡s rÃ¡pido."   
âœ… Diferenciable y computacionalmente lo suficientemente rÃ¡pida para usarse como loss.  
âŒ Si $\epsilon$ es muy grande, la mÃ©trica se vuelve demasiado borrosa y pierde la precisiÃ³n geomÃ©trica de la distancia Wasserstein verdadera.  

## G3. TÃ©rminos de difusiÃ³n (eliminaciÃ³n de ruido)
Usados en Diffusion Probabilistic Models (DDPMs). El objetivo es revertir un proceso de ruido gradual.

**Simple Diffusion**   
ğŸ—’ï¸ El modelo predice el ruido $\epsilon$ que fue agregado a la imagen $x_0$ en el paso de tiempo $t$.  
ğŸ’¡ "Te voy a mostrar una pantalla de TV con ruido. Decime exactamente quÃ© pÃ­xeles son ruido para que pueda restarlos y revelar la imagen de abajo."   
âœ… El entrenamiento es esencialmente un conjunto masivo de tareas de regresiÃ³n (MSE sobre ruido), lo cual es mucho mÃ¡s estable comparado con GANs.  
âŒ La inferencia es lenta, ya que generar una sola imagen requiere correr la red iterativamente (ej., 50 a 1000 veces) para eliminar el ruido paso a paso.  

**Denoising Score Matching**  
ğŸ—’ï¸ Optimiza el modelo para estimar la funciÃ³n de score (el gradiente de la log-densidad de los datos). Al moverse a lo largo del gradiente, se mueve desde un punto de datos ruidosos hacia un punto de datos limpios.  
ğŸ’¡ "Te tiran en un bosque con niebla. No sabÃ©s dÃ³nde estÃ¡ la cima de la montaÃ±a, pero si mirÃ¡s tus pies y pisÃ¡s donde el suelo va hacia arriba, eventualmente vas a llegar."   
âœ… Evita el problema intratable de calcular la constante de normalizaciÃ³n de la distribuciÃ³n de probabilidad.  
âŒ TÃ©cnicamente compleja de derivar e implementar comparada con el objetivo simplificado usado en Simple Diffusion.  

## G4. TÃ©rminos de guÃ­a auxiliar (basados en features)
En lugar de comparar pÃ­xeles crudos, estas losses comparan representaciones de alto nivel extraÃ­das por una red pre-entrenada.

**Perceptual**  
$$ L_{Perc} = || \phi(x) - \phi(\hat{x}) ||_2^2 $$
Donde $\phi$ es un extractor de features pre-entrenado.  
ğŸ—’ï¸ Compara los mapas de activaciÃ³n internos de una red pre-entrenada para las imÃ¡genes reales y generadas.  
ğŸ’¡ "No me importa si el pÃ­xel exacto coincide. Â¿La imagen parece un perro? Â¿Los bordes y texturas coinciden con la percepciÃ³n humana?"   
âœ… Correlaciona mucho mejor con el juicio visual humano que MSE, y proporciona texturas excelentes para transferencia de estilo y super-resoluciÃ³n.  
âŒ Depende de redes pre-entrenadas, por lo que puede fallar o producir artefactos si el dominio objetivo es vastamente diferente.  

**Style**  
$$ L_{Style} = || G(\phi(x)) - G(\phi(\hat{x})) ||_F^2 $$
Donde $G$ calcula la Matriz de Gramâ€”correlaciones entre features.  
ğŸ—’ï¸ Mide la correlaciÃ³n entre diferentes canales de features. Captura el "estilo" (textura, pinceladas, patrones de color) mientras descarta la estructura espacial.  
ğŸ’¡ "CapturÃ¡ la onda de Van Gogh pero no te preocupes por dÃ³nde estÃ¡n ubicados los Ã¡rboles."   
âœ… Desacopla explÃ­citamente la textura de la estructura, permitiendo la sÃ­ntesis de patrones artÃ­sticos complejos sin necesitar datos de entrenamiento pareados.  
âŒ No impone coherencia espacial, por lo que parches de textura pueden aparecer en ubicaciones semÃ¡nticamente incorrectas (ej., pinceladas apareciendo en el cielo en lugar de los Ã¡rboles).